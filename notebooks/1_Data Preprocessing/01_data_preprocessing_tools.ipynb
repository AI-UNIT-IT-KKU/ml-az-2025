{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37puETfgRzzg"
      },
      "source": [
        "# Part 1 - Data Preprocessing in Python\n",
        "This notebook explains the **first step of any Machine Learning project: Data Preprocessing**.  \n",
        "We will follow the workflow:\n",
        "\n",
        "1. Importing the dataset  \n",
        "2. Handling missing data  \n",
        "3. Encoding categorical variables  \n",
        "4. Splitting the dataset  \n",
        "5. Feature scaling  \n",
        "\n",
        "Each step includes:  \n",
        "- **What?** (what the step means)  \n",
        "- **Why?** (why we need it in ML)  \n",
        "- **How?** (how we apply it in Python, with code & outputs)  \n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rK9pPTVCq4_-"
      },
      "source": [
        "## Where We Are in the ML Workflow\n",
        "\n",
        "The **Machine Learning workflow** has three main phases:  \n",
        "1. Data Preprocessing  \n",
        "2. Modeling  \n",
        "3. Evaluation  \n",
        "\n",
        "In this notebook, we will focus on **Phase 1: Data Preprocessing**.  \n",
        "This is the most critical step ‚Äî if the data is not clean and well-prepared, even the most powerful algorithms will fail.  \n",
        "\n",
        "üí° Think of it like cooking:  \n",
        "- If your ingredients are fresh and well-prepared, the dish will turn out delicious.  \n",
        "- If the ingredients are spoiled or messy, no recipe can save it.  \n",
        "\n",
        "That‚Äôs why **the success of your ML model starts with successful preprocessing**.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoRP98MpR-qj"
      },
      "source": [
        "## ‚≠ê Importing the Libraries\n",
        "\n",
        "Before starting, let‚Äôs understand what a **library** means in programming:  \n",
        "\n",
        "A **library** is a collection of **modules**.  \n",
        "Each module contains **functions** and **classes** that let us perform specific actions without writing everything from scratch.  \n",
        "\n",
        "üëâ Think of it like a toolbox:  \n",
        "- The **library** = the whole toolbox.  \n",
        "- The **module** = a set of related tools inside the box.  \n",
        "- A **function/class** = an individual tool you pick and use.  \n",
        "\n",
        "In Machine Learning projects, the most common libraries are:  \n",
        "\n",
        "- **NumPy** ‚Üí work with arrays (most ML models expect inputs as arrays).  \n",
        "- **Pandas** ‚Üí load, clean, and manipulate datasets (especially for preprocessing).  \n",
        "- **Matplotlib** ‚Üí visualize data with plots and charts.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "N-qiINBQSK2g"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RopL7tUZSQkT"
      },
      "source": [
        "## ‚≠ê Importing the Dataset\n",
        "\n",
        "<center>\n",
        "  <img src=\"../../docs/Dataset-1.png\" alt=\"Dataset Example\" width=\"400\"/>\n",
        "</center>\n",
        "\n",
        "**What:** We load the file `Data.csv` into a **DataFrame** (a table-like structure in Pandas).  \n",
        "**Why:** Machine Learning models expect two entities:  \n",
        "- **X** = independent variables (features) ‚Üí the inputs  \n",
        "- **y** = dependent variable (target) ‚Üí the output we want to predict  \n",
        "\n",
        "**How:** We use `iloc` for position-based indexing:  \n",
        "- `:` ‚Üí all rows  \n",
        "- `:-1` ‚Üí all columns except the last one  \n",
        "- `-1` ‚Üí only the last column  \n",
        "\n",
        "Finally, we convert them into **NumPy arrays** (with `.values`), because most ML algorithms in scikit-learn expect arrays rather than DataFrames.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WwEPNDWySTKm"
      },
      "outputs": [],
      "source": [
        "# 1) Load the dataset into a Pandas DataFrame (table with column names)\n",
        "dataset = pd.read_csv('Data.csv')\n",
        "\n",
        "# 2) Select features (X) and target (y) using iloc (position-based indexing)\n",
        "# iloc syntax: df.iloc[row_slice, column_slice]\n",
        "X = dataset.iloc[:, :-1].values\n",
        "y = dataset.iloc[:, -1].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCsz2yCebe1R",
        "outputId": "37a2f8da-8812-40cd-d36f-0de5aa073cfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 nan]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' nan 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ]
        }
      ],
      "source": [
        "print(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYrOQ43XcJR3",
        "outputId": "9e5c763a-311e-4276-e3a0-2d29b8a01330"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
          ]
        }
      ],
      "source": [
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQQwUtrHu809"
      },
      "source": [
        "**Notes**\n",
        "- `.iloc` selects columns/rows by **position** (0, 1, 2, ‚Ä¶).  \n",
        "- `.values` (or `.to_numpy()`) converts DataFrame/Series into NumPy arrays.  \n",
        "- If your target column is not the last one, you can use its name instead:  \n",
        "  `y = dataset['Purchased'].values`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhfKXNxlSabC"
      },
      "source": [
        "## ‚≠ê Taking care of missing data\n",
        "Real-world datasets usually contain **missing values**.  \n",
        "\n",
        "ML models cannot work with missing data directly. If we don‚Äôt handle it, training will fail.\n",
        "\n",
        "Some Options:  \n",
        "1. Delete rows (only if few missing values).  \n",
        "2. Replace missing values with statistical measures: mean, median, or mode.  \n",
        "\n",
        "We will use **SimpleImputer** from `sklearn` to replace missing values with the column mean.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7bCAmKd-fYL",
        "outputId": "26249d88-19c8-4810-b443-01b7a50efdc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Country    Age  Salary  Purchased\n",
            "0    False  False   False      False\n",
            "1    False  False   False      False\n",
            "2    False  False   False      False\n",
            "3    False  False   False      False\n",
            "4    False  False    True      False\n",
            "5    False  False   False      False\n",
            "6    False   True   False      False\n",
            "7    False  False   False      False\n",
            "8    False  False   False      False\n",
            "9    False  False   False      False\n",
            "Country      0\n",
            "Age          1\n",
            "Salary       1\n",
            "Purchased    0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Identify missing data (assumes that missing data is represented as NaN)\n",
        "print(dataset.isnull())\n",
        "# Print the number of missing entries in each column\n",
        "print(dataset.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "c93k7ipkSexq"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "# Create an imputer object that will replace missing values (NaN) with the column mean\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "\n",
        "# Fit the imputer on columns 1 and 2 (indexing in Python starts at 0, so [:, 1:3] means 2nd and 3rd columns)\n",
        "imputer.fit(X[:, 1:3])\n",
        "\n",
        "# Transform the data: replace NaN with the calculated mean values\n",
        "X[:, 1:3] = imputer.transform(X[:, 1:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UgLdMS_bjq_",
        "outputId": "f714dbda-62ed-4a61-c27c-0a220ea032c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 63777.77777777778]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' 38.77777777777778 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ]
        }
      ],
      "source": [
        "print(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8X-ZwdZ0OND"
      },
      "source": [
        "üîé **Result Explanation**\n",
        "\n",
        "- `fit()` ‚Üí calculates the mean of each selected column (does not change the data yet).  \n",
        "- `transform()` ‚Üí replaces the missing values (`NaN`) with those calculated means.  \n",
        "- `X[:, 1:3]` ‚Üí selects only the **2nd and 3rd columns** (Python slicing: start at index 1, stop before index 3).  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CriG6VzVSjcK"
      },
      "source": [
        "## ‚≠ê Encoding categorical data\n",
        "\n",
        "**Problem:**  \n",
        "Machine Learning models can only understand numbers, not text.  \n",
        "But in our dataset, some columns contain text (like country names, or Yes/No answers).  \n",
        "\n",
        "**Solution:**  \n",
        "We convert text ‚Üí numbers using special encoders.  \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhSpdQWeSsFh"
      },
      "source": [
        "### Encoding the Independent Variable\n",
        "#### 1. Encoding X (features)  \n",
        "Suppose we have a column with countries:  \n",
        "`France`, `Spain`, `Germany`  \n",
        "\n",
        "‚ùå Bad idea: France=0, Spain=1, Germany=2  \n",
        "- This makes it look like *Germany (2)* > *Spain (1)* > *France (0)*, which is not true.  \n",
        "- The model would **assume an order or ranking** between the countries.  \n",
        "- This false ordering can confuse the algorithm and **reduce the model‚Äôs quality and accuracy**.  \n",
        "\n",
        "\n",
        "‚úÖ Better idea: **One Hot Encoding**  \n",
        "- Create a new column for each country.  \n",
        "- Example:  \n",
        "\n",
        "| Country   | France | Spain | Germany |  \n",
        "|-----------|--------|-------|---------|  \n",
        "| France    |   1    |   0   |   0     |  \n",
        "| Spain     |   0    |   1   |   0     |  \n",
        "| Germany   |   0    |   0   |   1     |  \n",
        "\n",
        "This way, the model sees only \"yes/no\" (1 or 0), no fake ordering.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5hwuVddlSwVi"
      },
      "outputs": [],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# One Hot Encoding for the first column (Country)\n",
        "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough' # keep other columns as they are\n",
        "                       )\n",
        "\n",
        "# result will be a NumPy array with new encoded columns\n",
        "X = np.array(ct.fit_transform(X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7QspewyeBfx",
        "outputId": "a951c620-9f90-4e7f-ffba-4430426723f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [0.0 1.0 0.0 30.0 54000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 35.0 58000.0]\n",
            " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ]
        }
      ],
      "source": [
        "print(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXh8oVSITIc6"
      },
      "source": [
        "### Encoding the Dependent Variable\n",
        "\n",
        "#### 2. Encoding y (target)  \n",
        "Our target column is binary (Yes / No).  \n",
        "We can simply replace it with numbers:  \n",
        "\n",
        "- Yes ‚Üí 1  \n",
        "- No ‚Üí 0  \n",
        "\n",
        "This works fine, because there are only two categories.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "XgHCShVyTOYY"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Label Encoding for the target (Yes/No)\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyhY8-gPpFCa",
        "outputId": "9a733c7d-967f-41bf-acd7-7ef34256b0db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 1 0 0 1 1 0 1 0 1]\n"
          ]
        }
      ],
      "source": [
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZHY0PQ83qcn"
      },
      "source": [
        "üîé **Result Explanation**\n",
        "\n",
        "- **OneHotEncoder** ‚Üí turns text categories into multiple binary columns (0/1).  \n",
        "- **remainder='passthrough'** ‚Üí keep the other columns as they are.  \n",
        "- **LabelEncoder** ‚Üí converts Yes/No into numbers (Yes=1, No=0).  \n",
        "\n",
        "üëâ Now, both X and y are numeric, ready for ML models.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb_vcgm3qZKW"
      },
      "source": [
        "## ‚≠ê Splitting the Dataset  \n",
        "\n",
        "We divide the dataset into two parts:  \n",
        "- **Training set (80%)** ‚Üí used to *teach* the model.  \n",
        "- **Test set (20%)** ‚Üí used to *evaluate* the model on new, unseen data.  \n",
        "\n",
        "Why do we do this?  \n",
        "- To make sure the model can **generalize** (perform well on new data), not just memorize the training data.  \n",
        "\n",
        "<center>\n",
        "  <img src=\"../../docs/Train & Test sets Spliting.png\"  width=\"400\"/>\n",
        "</center>\n",
        "\n",
        "---\n",
        "\n",
        "‚ùì **Important Question:**  \n",
        "Should we split before or after scaling?  \n",
        "\n",
        "üëâ **Answer:** Always split **before scaling**.  \n",
        "\n",
        "---\n",
        "\n",
        "### üîé Why? (Data Leakage)  \n",
        "\n",
        "If we scale *before* splitting:  \n",
        "- The scaler would calculate the mean/standard deviation using **all the data (including the test set)**.  \n",
        "- This means some information from the test set ‚Äúleaks‚Äù into the training process.  \n",
        "- As a result, the test set is no longer *truly unseen*.  \n",
        "- The model may look like it performs better than it really does (fake accuracy).  \n",
        "\n",
        "‚úÖ Correct way:  \n",
        "1. Split into train and test sets.  \n",
        "2. Fit the scaler **only on the training set** (calculate statistics).  \n",
        "3. Apply the same transformation to the test set **(we will see this in the next step: Feature Scaling)**.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "pXgA6CzlqbCl"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the dataset: 80% training, 20% testing\n",
        "# This 80/20 split is not mandatory, but it's the most common practice.\n",
        "# Why? ‚Üí We want to give the model as much data as possible for training,\n",
        "# while still keeping a separate portion to test how well it generalizes.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
        "\n",
        "# X_train, y_train ‚Üí used to build the model\n",
        "# X_test, y_test ‚Üí used to check if the model generalizes well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuwQhFdKrYTM",
        "outputId": "926c4b59-670c-4cbe-8333-3b397c6980a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 35.0 58000.0]]\n"
          ]
        }
      ],
      "source": [
        "print(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUrX_Tvcrbi4",
        "outputId": "f307ed08-ec8c-4fd2-bc2c-3797f2b8a8b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.0 1.0 0.0 30.0 54000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ]
        }
      ],
      "source": [
        "print(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSMHiIsWreQY",
        "outputId": "8990a085-97ef-4f5d-fe8b-83ee5c81cff6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 1 0 0 1 1 0 1]\n"
          ]
        }
      ],
      "source": [
        "print(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_tW7H56rgtW",
        "outputId": "e3f9224a-889f-45c2-e197-dfdddc37f571"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 1]\n"
          ]
        }
      ],
      "source": [
        "print(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpGqbS4TqkIR"
      },
      "source": [
        "## ‚≠ê Feature Scaling  \n",
        "\n",
        "**The Problem:**  \n",
        "In datasets, different features can have very different ranges.  \n",
        "- Example:  \n",
        "  - Age ‚Üí values like 20, 35, 50, 100  \n",
        "  - Salary ‚Üí values like 30,000, 50,000, 70,000  \n",
        "\n",
        "If we leave them like this, many ML algorithms will give **more importance** to the feature with larger values (Salary), and **ignore** the smaller one (Age).  \n",
        "üëâ This creates a **bias** in the model, leading to poor results.  \n",
        "\n",
        "---\n",
        "\n",
        "**The Goal of Scaling:**  \n",
        "- Put all features on a similar scale.  \n",
        "- Prevent one feature from *dominating* the others just because of its units.  \n",
        "- Make training faster and more stable.  \n",
        "\n",
        "---\n",
        "\n",
        "**Two Main Techniques:**  \n",
        "\n",
        "1. **Normalization**  \n",
        "   - Scales all values into the range [0, 1].  \n",
        "   - Best when data follows a *uniform distribution*.  \n",
        "  \n",
        "   \n",
        "   **Formula :**\n",
        "   $$\n",
        "   x' = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}\n",
        "   $$\n",
        "\n",
        "\n",
        "2. **Standardization**  \n",
        "   - Scales values so that the mean = 0 and standard deviation = 1.  \n",
        "   - Works well even when data is not uniformly distributed.  \n",
        "\n",
        "   **Formula :**\n",
        "   $$\n",
        "   z = \\frac{x - \\mu}{\\sigma}\n",
        "   $$\n",
        "\n",
        "   - This is the **most widely used** method in ML.  \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Before scaling, we assumed the result would look like this.\n",
        "<center>\n",
        "  <img src=\"../../docs/Feature Scaling.png\"  width=\"400\"/>\n",
        "</center>\n",
        "\n",
        "However, after applying scaling, we discovered that our assumption was wrong and that the correct result is actually like this.\n",
        "<center>\n",
        "  <img src=\"../../docs/Feature Scaling - after.png\"  width=\"400\"/>\n",
        "</center>\n",
        "\n",
        "- Therefore, we don‚Äôt want our model to fall into the same mistake.\n",
        "---\n",
        "\n",
        "‚ö†Ô∏è **Important Note:**  \n",
        "- Do **not** scale dummy variables (e.g., 0/1 values from OneHotEncoding).  \n",
        "- Scaling them would destroy their meaning and make them hard to interpret.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "AxjSUXFQqo-3"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "\n",
        "# Fit on training set ‚Üí calculate mean & std (statistics) from training data\n",
        "X_train[:, 3:] = sc.fit_transform(X_train[:, 3:])\n",
        "\n",
        "# Transform test set using the same statistics (no new fit here!)\n",
        "X_test[:, 3:] = sc.transform(X_test[:, 3:])\n",
        "\n",
        "# Now both train and test features are on the same scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWPET8ZdlMnu",
        "outputId": "af06df18-fba2-4eb1-8d10-821d0aa22f65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.0 0.0 1.0 -0.19159184384578545 -1.0781259408412425]\n",
            " [0.0 1.0 0.0 -0.014117293757057777 -0.07013167641635372]\n",
            " [1.0 0.0 0.0 0.566708506533324 0.633562432710455]\n",
            " [0.0 0.0 1.0 -0.30453019390224867 -0.30786617274297867]\n",
            " [0.0 0.0 1.0 -1.9018011447007988 -1.420463615551582]\n",
            " [1.0 0.0 0.0 1.1475343068237058 1.232653363453549]\n",
            " [0.0 1.0 0.0 1.4379472069688968 1.5749910381638885]\n",
            " [1.0 0.0 0.0 -0.7401495441200351 -0.5646194287757332]]\n"
          ]
        }
      ],
      "source": [
        "print(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTXykB_QlRjE",
        "outputId": "e38371e9-45bc-49bc-d2bd-c8e76b6f912d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.0 1.0 0.0 -1.4661817944830124 -0.9069571034860727]\n",
            " [1.0 0.0 0.0 -0.44973664397484414 0.2056403393225306]]\n"
          ]
        }
      ],
      "source": [
        "print(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cgu5Mcrx7nNj"
      },
      "source": [
        "üîé **Result Explanation**  \n",
        "- `fit_transform` on training data ‚Üí learns scaling parameters and applies them.  \n",
        "- `transform` on test data ‚Üí applies the same scaling, without leaking test info.  \n",
        "- Only scale **numeric continuous features**, not dummy variables.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcilSUp66ESq"
      },
      "source": [
        "## üí° Conclusion\n",
        "In this notebook, we learned how to:  \n",
        "1. Import & explore a dataset.  \n",
        "2. Handle missing values.  \n",
        "3. Encode categorical features.  \n",
        "4. Split dataset into training & testing.  \n",
        "5. Apply feature scaling.  \n",
        "\n",
        "‚úÖ Data preprocessing ensures that our dataset is **clean, consistent, and ready for ML models**.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
