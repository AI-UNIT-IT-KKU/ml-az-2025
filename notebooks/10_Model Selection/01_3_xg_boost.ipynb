{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FLud1n-3pVm"
      },
      "source": [
        "# XGBoost\n",
        "üéâ Congratulations! You‚Äôve reached the final step of your machine learning journey in this course.  \n",
        "And we‚Äôre finishing strong with one of the **most powerful algorithms** in the industry: **XGBoost**.\n",
        "\n",
        "Let‚Äôs see if this model can beat everything we‚Äôve built so far! üí™\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ What is XGBoost?\n",
        "\n",
        "**XGBoost (Extreme Gradient Boosting)** is an advanced machine learning algorithm designed for **speed and performance**. It works incredibly well for both **classification** and **regression** problems.\n",
        "\n",
        "Why is XGBoost so powerful?\n",
        "\n",
        "- Built-in regularization (prevents overfitting)\n",
        "- Fast, efficient, and scalable\n",
        "- Handles missing values automatically\n",
        "- Often wins machine learning competitions!\n",
        "\n",
        "---\n",
        "\n",
        "> üîÑ **XGBoost stands for \"Extreme Gradient Boosting\"** ‚Äî a cutting-edge implementation of the **Boosting** technique in Ensemble Learning.\n",
        "\n",
        "### üß© Why is XGBoost an Ensemble Model?\n",
        "\n",
        "XGBoost belongs to the family of **Ensemble Learning** algorithms. Specifically, it falls under:\n",
        "\n",
        "* ‚úÖ **Boosting**: A method where multiple weak learners (typically decision trees) are trained sequentially. Each new model tries to **fix the errors** made by the previous one.\n",
        "\n",
        "This makes XGBoost **different from Bagging methods** like Random Forest:\n",
        "\n",
        "| Feature           | Random Forest (Bagging) | XGBoost (Boosting)              |\n",
        "| ----------------- | ----------------------- | ------------------------------- |\n",
        "| Learning Strategy | Parallel                | Sequential                      |\n",
        "| Error Handling    | Voting/Averaging        | Focuses on previous errors      |\n",
        "| Regularization    | Not built-in            | Built-in L1 & L2 regularization |\n",
        "| Speed             | Fast in training        | Slower but often more accurate  |\n",
        "\n",
        "> üß† In short: XGBoost = multiple smart trees + error correction + regularization = **high-performance model** üí™\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sO8VPU6n3vES"
      },
      "source": [
        "## ‚≠ê Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clDSsF7P33NU"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGpwK5XD386E"
      },
      "source": [
        "## ‚≠ê Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zcksk88u4Ae8"
      },
      "source": [
        "dataset = pd.read_csv('Data.csv')\n",
        "X = dataset.iloc[:, :-1].values\n",
        "y = dataset.iloc[:, -1].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNn2RnST6_Q-"
      },
      "source": [
        "## ‚≠ê Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajhBL-er7Gry"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Y89ctGZ7Mcx"
      },
      "source": [
        "## ‚≠ê Training XGBoost on the Training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ude1J0E47SKN"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "classifier = XGBClassifier()\n",
        "classifier.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivqmubzW7dFJ"
      },
      "source": [
        "## ‚≠ê Making the Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUSZ3zm_7gRD"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "y_pred = classifier.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnbCjHgQ8XPn"
      },
      "source": [
        "## ‚≠ê Applying k-Fold Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYbfiITD8ZAz"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
        "print(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
        "print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}